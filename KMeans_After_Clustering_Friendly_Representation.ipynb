{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KMeans_After_Clustering_Friendly_Representation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdHCnHzVOtqE"
      },
      "outputs": [],
      "source": [
        "#Importing the libraries\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "import tqdm.autonotebook as tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Function\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.models import resnet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Arguments parser\n",
        "def parse():\n",
        "    parser = {}\n",
        "    parser[\"gpus\"] = '0'\n",
        "    parser[\"num_workers\"] = 8\n",
        "    args = parser\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args[\"gpus\"]\n",
        "    return parser"
      ],
      "metadata": {
        "id": "v2IloWR9PWjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AverageTracker():\n",
        "    def __init__(self):\n",
        "        self.step = 0\n",
        "        self.cur_avg = 0\n",
        "\n",
        "    def add(self, value):\n",
        "        self.cur_avg *= self.step / (self.step + 1)\n",
        "        self.cur_avg += value / (self.step + 1)\n",
        "        self.step += 1\n",
        "\n",
        "    def reset(self):\n",
        "        self.step = 0\n",
        "        self.cur_avg = 0\n",
        "\n",
        "    def avg(self):\n",
        "        return self.cur_avg.item()"
      ],
      "metadata": {
        "id": "85Y1sT5lPiRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CIFAR10(datasets.CIFAR10):\n",
        "    def __getitem__(self, index):\n",
        "        img, target = super().__getitem__(index)\n",
        "        return img, target, index"
      ],
      "metadata": {
        "id": "Ixbnh3uXPlB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class metrics:\n",
        "    ari = adjusted_rand_score\n",
        "    nmi = normalized_mutual_info_score\n",
        "\n",
        "    @staticmethod\n",
        "    def acc(y_true, y_pred):\n",
        "        y_true = y_true.astype(np.int64)\n",
        "        y_pred = y_pred.astype(np.int64)\n",
        "        assert y_pred.size == y_true.size\n",
        "        D = max(y_pred.max(), y_true.max()) + 1\n",
        "        w = np.zeros((D, D), dtype=np.int64)\n",
        "        for i in range(y_pred.size):\n",
        "            w[y_pred[i], y_true[i]] += 1\n",
        "        row, col = linear_sum_assignment(w.max() - w)\n",
        "        return sum([w[i, j] for i, j in zip(row, col)]) * 1.0 / y_pred.size"
      ],
      "metadata": {
        "id": "6UTFdvPTPr8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NonParametricClassifierOP(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, y, memory, params):\n",
        "        tau = params[0].item()\n",
        "        out = x.mm(memory.t())\n",
        "        out.div_(tau)\n",
        "        ctx.save_for_backward(x, memory, y, params)\n",
        "        return out\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        x, memory, y, params = ctx.saved_tensors\n",
        "        tau = params[0]\n",
        "        momentum = params[1]\n",
        "\n",
        "        grad_output.div_(tau)\n",
        "\n",
        "        grad_input = grad_output.mm(memory)\n",
        "        grad_input.resize_as_(x)\n",
        "\n",
        "        weight_pos = memory.index_select(0, y.view(-1)).resize_as_(x)\n",
        "        weight_pos.mul_(momentum)\n",
        "        weight_pos.add_(x.mul(1 - momentum))\n",
        "        w_norm = weight_pos.pow(2).sum(1, keepdim=True).pow(0.5)\n",
        "        updated_weight = weight_pos.div(w_norm)\n",
        "        memory.index_copy_(0, y, updated_weight)\n",
        "\n",
        "        return grad_input, None, None, None, None"
      ],
      "metadata": {
        "id": "i_LZZzDmPv8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NonParametricClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, tau=1.0, momentum=0.5):\n",
        "        super(NonParametricClassifier, self).__init__()\n",
        "        self.register_buffer('params', torch.tensor([tau, momentum]))\n",
        "        stdv = 1. / np.sqrt(input_dim / 3.)\n",
        "        self.register_buffer(\n",
        "            'memory',\n",
        "            torch.rand(output_dim, input_dim).mul_(2 * stdv).add_(-stdv))\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        out = NonParametricClassifierOP.apply(x, y, self.memory, self.params)\n",
        "        return out"
      ],
      "metadata": {
        "id": "WXFFrXiVPyAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Normalize(nn.Module):\n",
        "    def __init__(self, power=2):\n",
        "        super().__init__()\n",
        "        self.power = power\n",
        "\n",
        "    def forward(self, x):\n",
        "        norm = x.pow(self.power).sum(1, keepdim=True).pow(1. / self.power)\n",
        "        out = x.div(norm)\n",
        "        return out"
      ],
      "metadata": {
        "id": "izIPH2foP0at"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_clustering_metrics(npc, train_loader):\n",
        "    trainFeatures = npc.memory\n",
        "    z = trainFeatures.cpu().numpy()\n",
        "    y = np.array(train_loader.dataset.targets)\n",
        "    n_clusters = len(np.unique(y))\n",
        "    kmeans = KMeans(n_clusters=n_clusters, n_init=20)\n",
        "    y_pred = kmeans.fit_predict(z)\n",
        "    return metrics.acc(y, y_pred), metrics.nmi(y,\n",
        "                                               y_pred), metrics.ari(y, y_pred)"
      ],
      "metadata": {
        "id": "Uy5JYevUPorn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ResNet18(low_dim=128):\n",
        "    net = resnet.ResNet(resnet.BasicBlock, [2, 2, 2, 2], low_dim)\n",
        "    net.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                          stride=1, padding=1, bias=False)\n",
        "    net.maxpool = nn.Identity()\n",
        "    return net"
      ],
      "metadata": {
        "id": "S67YGTTAP4t1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Loss(nn.Module):\n",
        "    def __init__(self, tau2):\n",
        "        super().__init__()\n",
        "        self.tau2 = tau2\n",
        "\n",
        "    def forward(self, x, ff, y):\n",
        "        L_id = F.cross_entropy(x, y)\n",
        "        norm_ff = ff / (ff**2).sum(0, keepdim=True).sqrt()\n",
        "        coef_mat = torch.mm(norm_ff.t(), norm_ff)\n",
        "        coef_mat.div_(self.tau2)\n",
        "        a = torch.arange(coef_mat.size(0), device=coef_mat.device)\n",
        "        L_fd = F.cross_entropy(coef_mat, a)\n",
        "        return L_id, L_fd"
      ],
      "metadata": {
        "id": "MEQp2U_rP6on"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    args = parse()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(device)\n",
        "\n",
        "    tf = [\n",
        "        transforms.RandomResizedCrop(size=32,\n",
        "                                     scale=(0.2, 1.0),\n",
        "                                     ratio=(3 / 4, 4 / 3)),\n",
        "        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
        "        transforms.RandomGrayscale(p=0.2),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
        "                             std=[0.2470, 0.2435, 0.2616])\n",
        "    ]\n",
        "    transform = transforms.Compose(tf)\n",
        "\n",
        "    trainset = CIFAR10(root=\"~/.datasets\",\n",
        "                       train=True,\n",
        "                       download=True,\n",
        "                       transform=transform)\n",
        "    train_loader = torch.utils.data.DataLoader(trainset,\n",
        "                                               batch_size=128,\n",
        "                                               shuffle=True,\n",
        "                                               pin_memory=True,\n",
        "                                               num_workers=args[\"num_workers\"])\n",
        "\n",
        "    low_dim = 128\n",
        "    net = ResNet18(low_dim=low_dim)\n",
        "    norm = Normalize(2)\n",
        "    npc = NonParametricClassifier(input_dim=low_dim,\n",
        "                                  output_dim=len(trainset),\n",
        "                                  tau=1.0,\n",
        "                                  momentum=0.5)\n",
        "    loss = Loss(tau2=2.0)\n",
        "    net, norm = net.to(device), norm.to(device)\n",
        "    npc, loss = npc.to(device), loss.to(device)\n",
        "    optimizer = torch.optim.SGD(net.parameters(),\n",
        "                                lr=0.03,\n",
        "                                momentum=0.9,\n",
        "                                weight_decay=5e-4,\n",
        "                                nesterov=False,\n",
        "                                dampening=0)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
        "                                                        [600, 950, 1300, 1650],\n",
        "                                                        gamma=0.1)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        net = torch.nn.DataParallel(net,\n",
        "                                    device_ids=range(len(\n",
        "                                        args[\"gpus\"].split(\",\"))))\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    trackers = {n: AverageTracker() for n in [\"loss\", \"loss_id\", \"loss_fd\"]}\n",
        "    with tqdm.trange(25) as epoch_bar:\n",
        "        for epoch in epoch_bar:\n",
        "            net.train()\n",
        "            for batch_idx, (inputs, _,\n",
        "                            indexes) in enumerate(tqdm.tqdm(train_loader)):\n",
        "                optimizer.zero_grad()\n",
        "                inputs = inputs.to(device, dtype=torch.float32, non_blocking=True)\n",
        "                indexes = indexes.to(device, non_blocking=True)\n",
        "                features = norm(net(inputs))\n",
        "                outputs = npc(features, indexes)\n",
        "                loss_id, loss_fd = loss(outputs, features, indexes)\n",
        "                tot_loss = loss_id + loss_fd\n",
        "                tot_loss.backward()\n",
        "                optimizer.step()\n",
        "                # track loss\n",
        "                trackers[\"loss\"].add(tot_loss)\n",
        "                trackers[\"loss_id\"].add(loss_id)\n",
        "                trackers[\"loss_fd\"].add(loss_fd)\n",
        "            lr_scheduler.step()\n",
        "\n",
        "            # logging\n",
        "            postfix = {name: t.avg() for name, t in trackers.items()}\n",
        "            epoch_bar.set_postfix(**postfix)\n",
        "            for t in trackers.values():\n",
        "                t.reset()\n",
        "\n",
        "            # check clustering acc\n",
        "            acc, nmi, ari = check_clustering_metrics(npc, train_loader)\n",
        "            print(\"Epoch:{} Kmeans ACC, NMI, ARI = {}, {}, {}\".format(epoch+1, acc, nmi, ari))"
      ],
      "metadata": {
        "id": "7hJhEY5WPcFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Driver cell\n",
        "main()"
      ],
      "metadata": {
        "id": "EIjooY-FPfVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "1. https://arxiv.org/abs/2106.00131\n",
        "2. https://github.com/TTN-YKK/Clustering_friendly_representation_learning"
      ],
      "metadata": {
        "id": "u2auJFW80DL4"
      }
    }
  ]
}
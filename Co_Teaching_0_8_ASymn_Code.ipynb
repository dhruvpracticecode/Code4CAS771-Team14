{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Co_Teaching_0_8_ASymn_Code.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Importing the libraries\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import os\n",
        "import os.path\n",
        "import copy\n",
        "import hashlib\n",
        "import errno\n",
        "from numpy.testing import assert_array_almost_equal'\n",
        "import math\n",
        "import torch.nn.init as init \n",
        "import torch.optim as optim\n",
        "from __future__ import print_function\n",
        "from PIL import Image\n",
        "import sys\n",
        "import torchvision.transforms as transforms\n",
        "import argparse, sys\n",
        "import datetime\n",
        "import shutil"
      ],
      "metadata": {
        "id": "tSqgj_VPuFs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UucTMpoHy-ng"
      },
      "outputs": [],
      "source": [
        "#Loss function for Co-Teaching\n",
        "def loss_coteaching(y_1, y_2, t, forget_rate, ind, noise_or_not):\n",
        "    loss_1 = F.cross_entropy(y_1, t, reduce = False)\n",
        "    ind_1_sorted = np.argsort(loss_1.data.cpu())\n",
        "    loss_1_sorted = loss_1[ind_1_sorted]\n",
        "\n",
        "    loss_2 = F.cross_entropy(y_2, t, reduce = False)\n",
        "    ind_2_sorted = np.argsort(loss_2.data.cpu())\n",
        "    loss_2_sorted = loss_2[ind_2_sorted]\n",
        "\n",
        "    remember_rate = 1 - forget_rate\n",
        "    num_remember = int(remember_rate * len(loss_1_sorted))\n",
        "\n",
        "    pure_ratio_1 = np.sum(noise_or_not[ind[ind_1_sorted[:num_remember]]])/float(num_remember)\n",
        "    pure_ratio_2 = np.sum(noise_or_not[ind[ind_2_sorted[:num_remember]]])/float(num_remember)\n",
        "\n",
        "    ind_1_update=ind_1_sorted[:num_remember]\n",
        "    ind_2_update=ind_2_sorted[:num_remember]\n",
        "    # exchange\n",
        "    loss_1_update = F.cross_entropy(y_1[ind_2_update], t[ind_2_update])\n",
        "    loss_2_update = F.cross_entropy(y_2[ind_1_update], t[ind_1_update])\n",
        "\n",
        "    return torch.sum(loss_1_update)/num_remember, torch.sum(loss_2_update)/num_remember, pure_ratio_1, pure_ratio_2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining the back-end CNN model\n",
        "def call_bn(bn, x):\n",
        "    return bn(x)\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, input_channel=3, n_outputs=10, dropout_rate=0.25, top_bn=False):\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.top_bn = top_bn\n",
        "        super(CNN, self).__init__()\n",
        "        self.c1=nn.Conv2d(input_channel,128,kernel_size=3,stride=1, padding=1)\n",
        "        self.c2=nn.Conv2d(128,128,kernel_size=3,stride=1, padding=1)\n",
        "        self.c3=nn.Conv2d(128,256,kernel_size=3,stride=1, padding=1)\n",
        "        self.c4=nn.Conv2d(256,512,kernel_size=3,stride=1, padding=0)\n",
        "        self.c5=nn.Conv2d(512,256,kernel_size=3,stride=1, padding=0)\n",
        "        self.c6=nn.Conv2d(256,128,kernel_size=3,stride=1, padding=0)\n",
        "        self.l_c1=nn.Linear(128,n_outputs)\n",
        "        self.bn1=nn.BatchNorm2d(128)\n",
        "        self.bn2=nn.BatchNorm2d(128)\n",
        "        self.bn3=nn.BatchNorm2d(256)\n",
        "        self.bn4=nn.BatchNorm2d(512)\n",
        "        self.bn5=nn.BatchNorm2d(256)\n",
        "        self.bn6=nn.BatchNorm2d(128)\n",
        "\n",
        "    def forward(self, x,):\n",
        "        h=x\n",
        "        h=self.c1(h)\n",
        "        h=F.leaky_relu(call_bn(self.bn1, h), negative_slope=0.01)\n",
        "        h=self.c2(h)\n",
        "        h=F.leaky_relu(call_bn(self.bn2, h), negative_slope=0.01)\n",
        "        h=self.c3(h)\n",
        "        h=F.leaky_relu(call_bn(self.bn3, h), negative_slope=0.01)\n",
        "        h=F.max_pool2d(h, kernel_size=2, stride=2)\n",
        "        h=F.dropout2d(h, p=self.dropout_rate)\n",
        "\n",
        "        h=self.c4(h)\n",
        "        h=F.leaky_relu(call_bn(self.bn4, h), negative_slope=0.01)\n",
        "        h=self.c5(h)\n",
        "        h=F.leaky_relu(call_bn(self.bn5, h), negative_slope=0.01)\n",
        "        h=self.c6(h)\n",
        "        h=F.leaky_relu(call_bn(self.bn6, h), negative_slope=0.01)\n",
        "        h=F.max_pool2d(h, kernel_size=2, stride=2)\n",
        "        h=F.dropout2d(h, p=self.dropout_rate)\n",
        "        h=F.avg_pool2d(h, kernel_size=h.data.shape[2])\n",
        "\n",
        "        h = h.view(h.size(0), h.size(1))\n",
        "        logit=self.l_c1(h)\n",
        "        if self.top_bn:\n",
        "            logit=call_bn(self.bn_c1, logit)\n",
        "        return logit"
      ],
      "metadata": {
        "id": "FEr7WL2uzM4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Utility Files\n",
        "def check_integrity(fpath, md5):\n",
        "    if not os.path.isfile(fpath):\n",
        "        return False\n",
        "    md5o = hashlib.md5()\n",
        "    with open(fpath, 'rb') as f:\n",
        "        # read in 1MB chunks\n",
        "        for chunk in iter(lambda: f.read(1024 * 1024), b''):\n",
        "            md5o.update(chunk)\n",
        "    md5c = md5o.hexdigest()\n",
        "    if md5c != md5:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def download_url(url, root, filename, md5):\n",
        "    from six.moves import urllib\n",
        "    root = os.path.expanduser(root)\n",
        "    fpath = os.path.join(root, filename)\n",
        "    try:\n",
        "        os.makedirs(root)\n",
        "    except OSError as e:\n",
        "        if e.errno == errno.EEXIST:\n",
        "            pass\n",
        "        else:\n",
        "            raise\n",
        "    # downloads file\n",
        "    if os.path.isfile(fpath) and check_integrity(fpath, md5):\n",
        "        print('Using downloaded and verified file: ' + fpath)\n",
        "    else:\n",
        "        try:\n",
        "            print('Downloading ' + url + ' to ' + fpath)\n",
        "            urllib.request.urlretrieve(url, fpath)\n",
        "        except:\n",
        "            if url[:5] == 'https':\n",
        "                url = url.replace('https:', 'http:')\n",
        "                print('Failed download. Trying https -> http instead.'\n",
        "                      ' Downloading ' + url + ' to ' + fpath)\n",
        "                urllib.request.urlretrieve(url, fpath)\n",
        "\n",
        "def list_dir(root, prefix=False):\n",
        "    \"\"\"List all directories at a given root\n",
        "    Args:\n",
        "        root (str): Path to directory whose folders need to be listed\n",
        "        prefix (bool, optional): If true, prepends the path to each result, otherwise\n",
        "            only returns the name of the directories found\n",
        "    \"\"\"\n",
        "    root = os.path.expanduser(root)\n",
        "    directories = list(\n",
        "        filter(\n",
        "            lambda p: os.path.isdir(os.path.join(root, p)),\n",
        "            os.listdir(root)\n",
        "        )\n",
        "    )\n",
        "    if prefix is True:\n",
        "        directories = [os.path.join(root, d) for d in directories]\n",
        "    return directories\n",
        "\n",
        "def list_files(root, suffix, prefix=False):\n",
        "    \"\"\"List all files ending with a suffix at a given root\n",
        "    Args:\n",
        "        root (str): Path to directory whose folders need to be listed\n",
        "        suffix (str or tuple): Suffix of the files to match, e.g. '.png' or ('.jpg', '.png').\n",
        "            It uses the Python \"str.endswith\" method and is passed directly\n",
        "        prefix (bool, optional): If true, prepends the path to each result, otherwise\n",
        "            only returns the name of the files found\n",
        "    \"\"\"\n",
        "    root = os.path.expanduser(root)\n",
        "    files = list(\n",
        "        filter(\n",
        "            lambda p: os.path.isfile(os.path.join(root, p)) and p.endswith(suffix),\n",
        "            os.listdir(root)\n",
        "        )\n",
        "    )\n",
        "    if prefix is True:\n",
        "        files = [os.path.join(root, d) for d in files]\n",
        "    return files\n",
        "\n",
        "# basic function\n",
        "def multiclass_noisify(y, P, random_state=0):\n",
        "    \"\"\" Flip classes according to transition probability matrix T.\n",
        "    It expects a number between 0 and the number of classes - 1.\n",
        "    \"\"\"\n",
        "    print(np.max(y), P.shape[0])\n",
        "    assert P.shape[0] == P.shape[1]\n",
        "    assert np.max(y) < P.shape[0]\n",
        "\n",
        "    # row stochastic matrix\n",
        "    assert_array_almost_equal(P.sum(axis=1), np.ones(P.shape[1]))\n",
        "    assert (P >= 0.0).all()\n",
        "\n",
        "    m = y.shape[0]\n",
        "    print(m)\n",
        "    new_y = y.copy()\n",
        "    flipper = np.random.RandomState(random_state)\n",
        "\n",
        "    for idx in np.arange(m):\n",
        "        i = y[idx]\n",
        "        # draw a vector with only an 1\n",
        "        flipped = flipper.multinomial(1, P[i, :][0], 1)[0]\n",
        "        new_y[idx] = np.where(flipped == 1)[0]\n",
        "    return new_y\n",
        "\n",
        "# noisify_pairflip call the function \"multiclass_noisify\"\n",
        "def noisify_pairflip(y_train, noise, random_state=None, nb_classes=10):\n",
        "    \"\"\"mistakes:\n",
        "        flip in the pair\n",
        "    \"\"\"\n",
        "    P = np.eye(nb_classes)\n",
        "    n = noise\n",
        "    if n > 0.0:\n",
        "        # 0 -> 1\n",
        "        P[0, 0], P[0, 1] = 1. - n, n\n",
        "        for i in range(1, nb_classes-1):\n",
        "            P[i, i], P[i, i + 1] = 1. - n, n\n",
        "        P[nb_classes-1, nb_classes-1], P[nb_classes-1, 0] = 1. - n, n\n",
        "\n",
        "        y_train_noisy = multiclass_noisify(y_train, P=P,\n",
        "                                           random_state=random_state)\n",
        "        actual_noise = (y_train_noisy != y_train).mean()\n",
        "        assert actual_noise > 0.0\n",
        "        print('Actual noise %.2f' % actual_noise)\n",
        "        y_train = y_train_noisy\n",
        "    print(P)\n",
        "    return y_train, actual_noise\n",
        "\n",
        "def noisify_multiclass_symmetric(y_train, noise, random_state=None, nb_classes=10):\n",
        "    \"\"\"mistakes:\n",
        "        flip in the symmetric way\n",
        "    \"\"\"\n",
        "    P = np.ones((nb_classes, nb_classes))\n",
        "    n = noise\n",
        "    P = (n / (nb_classes - 1)) * P\n",
        "    if n > 0.0:\n",
        "        # 0 -> 1\n",
        "        P[0, 0] = 1. - n\n",
        "        for i in range(1, nb_classes-1):\n",
        "            P[i, i] = 1. - n\n",
        "        P[nb_classes-1, nb_classes-1] = 1. - n\n",
        "\n",
        "        y_train_noisy = multiclass_noisify(y_train, P=P,\n",
        "                                           random_state=random_state)\n",
        "        actual_noise = (y_train_noisy != y_train).mean()\n",
        "        assert actual_noise > 0.0\n",
        "        print('Actual noise %.2f' % actual_noise)\n",
        "        y_train = y_train_noisy\n",
        "    print(P)\n",
        "    return y_train, actual_noise\n",
        "\n",
        "def noisify(dataset, nb_classes=10, train_labels=None, noise_type=None, noise_rate=0, random_state=0):\n",
        "    if noise_type == 'pairflip':\n",
        "        train_noisy_labels, actual_noise_rate = noisify_pairflip(train_labels, noise_rate, random_state=0, nb_classes=nb_classes)\n",
        "    if noise_type == 'symmetric':\n",
        "        train_noisy_labels, actual_noise_rate = noisify_multiclass_symmetric(train_labels, noise_rate, random_state=0, nb_classes=nb_classes)\n",
        "    return train_noisy_labels, actual_noise_rate"
      ],
      "metadata": {
        "id": "Tcw4I68f0DfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Code for CIFAR-10 dataset\n",
        "if sys.version_info[0] == 2:\n",
        "    import cPickle as pickle\n",
        "else:\n",
        "    import pickle\n",
        "\n",
        "import torch.utils.data as data\n",
        "\n",
        "class CIFAR10(data.Dataset):\n",
        "    \"\"\"`CIFAR10 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset.\n",
        "    Args:\n",
        "        root (string): Root directory of dataset where directory\n",
        "            ``cifar-10-batches-py`` exists or will be saved to if download is set to True.\n",
        "        train (bool, optional): If True, creates dataset from training set, otherwise\n",
        "            creates from test set.\n",
        "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
        "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
        "        target_transform (callable, optional): A function/transform that takes in the\n",
        "            target and transforms it.\n",
        "        download (bool, optional): If true, downloads the dataset from the internet and\n",
        "            puts it in root directory. If dataset is already downloaded, it is not\n",
        "            downloaded again.\n",
        "    \"\"\"\n",
        "    base_folder = 'cifar-10-batches-py'\n",
        "    url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
        "    filename = \"cifar-10-python.tar.gz\"\n",
        "    tgz_md5 = 'c58f30108f718f92721af3b95e74349a'\n",
        "    train_list = [\n",
        "        ['data_batch_1', 'c99cafc152244af753f735de768cd75f'],\n",
        "        ['data_batch_2', 'd4bba439e000b95fd0a9bffe97cbabec'],\n",
        "        ['data_batch_3', '54ebc095f3ab1f0389bbae665268c751'],\n",
        "        ['data_batch_4', '634d18415352ddfa80567beed471001a'],\n",
        "        ['data_batch_5', '482c414d41f54cd18b22e5b47cb7c3cb'],\n",
        "    ]\n",
        "\n",
        "    test_list = [\n",
        "        ['test_batch', '40351d587109b95175f43aff81a1287e'],\n",
        "    ]\n",
        "\n",
        "    def __init__(self, root, train=True,\n",
        "                 transform=None, target_transform=None,\n",
        "                 download=False,\n",
        "                 noise_type=None, noise_rate=0.2, random_state=0):\n",
        "        self.root = os.path.expanduser(root)\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.train = train  # training set or test set\n",
        "        self.dataset='cifar10'\n",
        "        self.noise_type=noise_type\n",
        "        self.nb_classes=10\n",
        "\n",
        "        if download:\n",
        "            self.download()\n",
        "\n",
        "        if not self._check_integrity():\n",
        "            raise RuntimeError('Dataset not found or corrupted.' +\n",
        "                               ' You can use download=True to download it')\n",
        "\n",
        "        # now load the picked numpy arrays\n",
        "        if self.train:\n",
        "            self.train_data = []\n",
        "            self.train_labels = []\n",
        "            for fentry in self.train_list:\n",
        "                f = fentry[0]\n",
        "                file = os.path.join(self.root, self.base_folder, f)\n",
        "                fo = open(file, 'rb')\n",
        "                if sys.version_info[0] == 2:\n",
        "                    entry = pickle.load(fo)\n",
        "                else:\n",
        "                    entry = pickle.load(fo, encoding='latin1')\n",
        "                self.train_data.append(entry['data'])\n",
        "                if 'labels' in entry:\n",
        "                    self.train_labels += entry['labels']\n",
        "                else:\n",
        "                    self.train_labels += entry['fine_labels']\n",
        "                fo.close()\n",
        "\n",
        "            self.train_data = np.concatenate(self.train_data)\n",
        "            self.train_data = self.train_data.reshape((50000, 3, 32, 32))\n",
        "            self.train_data = self.train_data.transpose((0, 2, 3, 1))  # convert to HWC\n",
        "            #if noise_type is not None:\n",
        "            if noise_type !='clean':\n",
        "                # noisify train data\n",
        "                self.train_labels=np.asarray([[self.train_labels[i]] for i in range(len(self.train_labels))])\n",
        "                self.train_noisy_labels, self.actual_noise_rate = noisify(dataset=self.dataset, train_labels=self.train_labels, noise_type=noise_type, noise_rate=noise_rate, random_state=random_state, nb_classes=self.nb_classes)\n",
        "                self.train_noisy_labels=[i[0] for i in self.train_noisy_labels]\n",
        "                _train_labels=[i[0] for i in self.train_labels]\n",
        "                self.noise_or_not = np.transpose(self.train_noisy_labels)==np.transpose(_train_labels)\n",
        "        else:\n",
        "            f = self.test_list[0][0]\n",
        "            file = os.path.join(self.root, self.base_folder, f)\n",
        "            fo = open(file, 'rb')\n",
        "            if sys.version_info[0] == 2:\n",
        "                entry = pickle.load(fo)\n",
        "            else:\n",
        "                entry = pickle.load(fo, encoding='latin1')\n",
        "            self.test_data = entry['data']\n",
        "            if 'labels' in entry:\n",
        "                self.test_labels = entry['labels']\n",
        "            else:\n",
        "                self.test_labels = entry['fine_labels']\n",
        "            fo.close()\n",
        "            self.test_data = self.test_data.reshape((10000, 3, 32, 32))\n",
        "            self.test_data = self.test_data.transpose((0, 2, 3, 1))  # convert to HWC\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): Index\n",
        "        Returns:\n",
        "            tuple: (image, target) where target is index of the target class.\n",
        "        \"\"\"\n",
        "        if self.train:\n",
        "            if self.noise_type !='clean':\n",
        "                img, target = self.train_data[index], self.train_noisy_labels[index]\n",
        "            else:\n",
        "                img, target = self.train_data[index], self.train_labels[index]\n",
        "        else:\n",
        "            img, target = self.test_data[index], self.test_labels[index]\n",
        "\n",
        "        # doing this so that it is consistent with all other datasets\n",
        "        # to return a PIL Image\n",
        "        img = Image.fromarray(img)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return img, target, index\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.train:\n",
        "            return len(self.train_data)\n",
        "        else:\n",
        "            return len(self.test_data)\n",
        "\n",
        "    def _check_integrity(self):\n",
        "        root = self.root\n",
        "        for fentry in (self.train_list + self.test_list):\n",
        "            filename, md5 = fentry[0], fentry[1]\n",
        "            fpath = os.path.join(root, self.base_folder, filename)\n",
        "            if not check_integrity(fpath, md5):\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def download(self):\n",
        "        import tarfile\n",
        "\n",
        "        if self._check_integrity():\n",
        "            print('Files already downloaded and verified')\n",
        "            return\n",
        "\n",
        "        root = self.root\n",
        "        download_url(self.url, root, self.filename, self.tgz_md5)\n",
        "\n",
        "        # extract file\n",
        "        cwd = os.getcwd()\n",
        "        tar = tarfile.open(os.path.join(root, self.filename), \"r:gz\")\n",
        "        os.chdir(root)\n",
        "        tar.extractall()\n",
        "        tar.close()\n",
        "        os.chdir(cwd)\n",
        "\n",
        "    def __repr__(self):\n",
        "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
        "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
        "        tmp = 'train' if self.train is True else 'test'\n",
        "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
        "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
        "        tmp = '    Transforms (if any): '\n",
        "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
        "        tmp = '    Target Transforms (if any): '\n",
        "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
        "        return fmt_str"
      ],
      "metadata": {
        "id": "KA7cfsKKz3Lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining the parameters for the model\n",
        "parser = {}\n",
        "parser['lr'] = 0.001\n",
        "parser['result_dir'] = \"results/\"\n",
        "parser['noise_rate'] = 0.8\n",
        "parser['forget_rate'] = None\n",
        "parser['noise_type'] = \"symmetric\"\n",
        "parser['num_gradual'] = 10\n",
        "parser['exponent'] = 1\n",
        "parser['top_bn'] = \"store_true\"\n",
        "parser['dataset'] = \"cifar10\"\n",
        "parser['n_epoch'] = 25\n",
        "parser['seed'] = 1\n",
        "parser['print_freq'] = 50\n",
        "parser['num_workers'] = 4\n",
        "parser['num_iter_per_epoch'] = 400\n",
        "parser['epoch_decay_start'] = 80\n",
        "\n",
        "args = parser"
      ],
      "metadata": {
        "id": "jY9I1SRXzV7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seed\n",
        "torch.manual_seed(args[\"seed\"])\n",
        "torch.cuda.manual_seed(args[\"seed\"])\n",
        "\n",
        "# Hyper Parameters\n",
        "batch_size = 128\n",
        "learning_rate = args[\"lr\"]\n",
        "\n",
        "# load dataset   \n",
        "if args[\"dataset\"]=='cifar10':\n",
        "    input_channel=3\n",
        "    num_classes=10\n",
        "    args[\"top_bn\"] = False\n",
        "    args[\"epoch_decay_start\"] = 80\n",
        "    args[\"n_epoch\"] = 25\n",
        "    train_dataset = CIFAR10(root='./data/',\n",
        "                                download=True,  \n",
        "                                train=True, \n",
        "                                transform=transforms.ToTensor(),\n",
        "                                noise_type=args[\"noise_type\"],\n",
        "                                noise_rate=args[\"noise_rate\"]\n",
        "                           )\n",
        "    \n",
        "    test_dataset = CIFAR10(root='./data/',\n",
        "                                download=True,  \n",
        "                                train=False, \n",
        "                                transform=transforms.ToTensor(),\n",
        "                                noise_type=args[\"noise_type\"],\n",
        "                                noise_rate=args[\"noise_rate\"]\n",
        "                          )\n",
        "\n",
        "\n",
        "if args[\"forget_rate\"] is None:\n",
        "    forget_rate=args[\"noise_rate\"]\n",
        "else:\n",
        "    forget_rate=args[\"forget_rate\"]\n",
        "\n",
        "noise_or_not = train_dataset.noise_or_not\n",
        "\n",
        "# Adjust learning rate and betas for Adam Optimizer\n",
        "mom1 = 0.9\n",
        "mom2 = 0.1\n",
        "alpha_plan = [learning_rate] * args[\"n_epoch\"]\n",
        "beta1_plan = [mom1] * args[\"n_epoch\"]\n",
        "for i in range(args[\"epoch_decay_start\"], args[\"n_epoch\"]):\n",
        "    alpha_plan[i] = float(args[\"n_epoch\"] - i) / (args[\"n_epoch\"] - args[\"epoch_decay_start\"]) * learning_rate\n",
        "    beta1_plan[i] = mom2\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr']=alpha_plan[epoch]\n",
        "        param_group['betas']=(beta1_plan[epoch], 0.999) # Only change beta1\n",
        "        \n",
        "# define drop rate schedule\n",
        "rate_schedule = np.ones(args[\"n_epoch\"])*forget_rate\n",
        "rate_schedule[:args[\"num_gradual\"]] = np.linspace(0, forget_rate**args[\"exponent\"], args[\"num_gradual\"])\n",
        "   \n",
        "save_dir = args[\"result_dir\"] +'/' +args[\"dataset\"]+'/coteaching/'\n",
        "\n",
        "if not os.path.exists(save_dir):\n",
        "    os.system('mkdir -p %s' % save_dir)\n",
        "\n",
        "model_str=args[\"dataset\"]+'_coteaching_'+args[\"noise_type\"]+'_'+str(args[\"noise_rate\"])\n",
        "\n",
        "txtfile=save_dir+\"/\"+model_str+\".txt\"\n",
        "nowTime=datetime.datetime.now().strftime('%Y-%m-%d-%H:%M:%S')\n",
        "if os.path.exists(txtfile):\n",
        "    os.system('mv %s %s' % (txtfile, txtfile+\".bak-%s\" % nowTime))\n",
        "\n",
        "\n",
        "def accuracy(logit, target, topk=(1,)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    output = F.softmax(logit, dim=1)\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].contiguous().view(-1).float().sum(0, keepdim=True)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res\n",
        "\n",
        "# Train the Model\n",
        "def train(train_loader,epoch, model1, optimizer1, model2, optimizer2):\n",
        "    print('Training %s...' % model_str)\n",
        "    pure_ratio_list=[]\n",
        "    pure_ratio_1_list=[]\n",
        "    pure_ratio_2_list=[]\n",
        "    \n",
        "    train_total=0\n",
        "    train_correct=0 \n",
        "    train_total2=0\n",
        "    train_correct2=0 \n",
        "\n",
        "    for i, (images, labels, indexes) in enumerate(train_loader):\n",
        "        ind=indexes.cpu().numpy().transpose()\n",
        "        if i>args[\"num_iter_per_epoch\"]:\n",
        "            break\n",
        "      \n",
        "        images = Variable(images).cuda()\n",
        "        labels = Variable(labels).cuda()\n",
        "        \n",
        "        # Forward + Backward + Optimize\n",
        "        logits1=model1(images)\n",
        "        prec1, _ = accuracy(logits1, labels, topk=(1, 5))\n",
        "        train_total+=1\n",
        "        train_correct+=prec1\n",
        "\n",
        "        logits2 = model2(images)\n",
        "        prec2, _ = accuracy(logits2, labels, topk=(1, 5))\n",
        "        train_total2+=1\n",
        "        train_correct2+=prec2\n",
        "        loss_1, loss_2, pure_ratio_1, pure_ratio_2 = loss_coteaching(logits1, logits2, labels, rate_schedule[epoch], ind, noise_or_not)\n",
        "        pure_ratio_1_list.append(100*pure_ratio_1)\n",
        "        pure_ratio_2_list.append(100*pure_ratio_2)\n",
        "\n",
        "        optimizer1.zero_grad()\n",
        "        loss_1.backward()\n",
        "        optimizer1.step()\n",
        "        optimizer2.zero_grad()\n",
        "        loss_2.backward()\n",
        "        optimizer2.step()\n",
        "        if (i+1) % args[\"print_freq\"] == 0:\n",
        "            print ('Epoch [%d/%d], Iter [%d/%d] Training Accuracy1: %.4F, Training Accuracy2: %.4f, Loss1: %.4f, Loss2: %.4f, Pure Ratio1: %.4f, Pure Ratio2 %.4f' \n",
        "                   %(epoch+1, args[\"n_epoch\"], i+1, len(train_dataset)//batch_size, prec1, prec2, loss_1.data, loss_2.data, np.sum(pure_ratio_1_list)/len(pure_ratio_1_list), np.sum(pure_ratio_2_list)/len(pure_ratio_2_list)))\n",
        "\n",
        "    train_acc1=float(train_correct)/float(train_total)\n",
        "    train_acc2=float(train_correct2)/float(train_total2)\n",
        "    return train_acc1, train_acc2, pure_ratio_1_list, pure_ratio_2_list\n",
        "\n",
        "# Evaluate the Model\n",
        "def evaluate(test_loader, model1, model2):\n",
        "    print('Evaluating %s...' % model_str)\n",
        "    model1 = model1.cuda()\n",
        "    model1.eval()    # Change model to 'eval' mode.\n",
        "    correct1 = 0\n",
        "    total1 = 0\n",
        "    for images, labels, _ in test_loader:\n",
        "        images = Variable(images).cuda()\n",
        "        logits1 = model1(images)\n",
        "        outputs1 = F.softmax(logits1, dim=1)\n",
        "        _, pred1 = torch.max(outputs1.data, 1)\n",
        "        total1 += labels.size(0)\n",
        "        correct1 += (pred1.cpu() == labels).sum()\n",
        "\n",
        "    model2 = model2.cuda()\n",
        "    model2.eval()    # Change model to 'eval' mode \n",
        "    correct2 = 0\n",
        "    total2 = 0\n",
        "    for images, labels, _ in test_loader:\n",
        "        images = Variable(images).cuda()\n",
        "        logits2 = model2(images)\n",
        "        outputs2 = F.softmax(logits2, dim=1)\n",
        "        _, pred2 = torch.max(outputs2.data, 1)\n",
        "        total2 += labels.size(0)\n",
        "        correct2 += (pred2.cpu() == labels).sum()\n",
        " \n",
        "    acc1 = 100*float(correct1)/float(total1)\n",
        "    acc2 = 100*float(correct2)/float(total2)\n",
        "    return acc1, acc2\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Data Loader (Input Pipeline)\n",
        "    print('loading dataset...')\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                               batch_size=batch_size, \n",
        "                                               num_workers=args[\"num_workers\"],\n",
        "                                               drop_last=True,\n",
        "                                               shuffle=True)\n",
        "    \n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                              batch_size=batch_size, \n",
        "                                              num_workers=args[\"num_workers\"],\n",
        "                                              drop_last=True,\n",
        "                                              shuffle=False)\n",
        "    # Define models\n",
        "    print('building model...')\n",
        "    cnn1 = CNN(input_channel=input_channel, n_outputs=num_classes)\n",
        "    cnn1.cuda()\n",
        "    print(cnn1.parameters)\n",
        "    optimizer1 = torch.optim.Adam(cnn1.parameters(), lr=learning_rate)\n",
        "    \n",
        "    cnn2 = CNN(input_channel=input_channel, n_outputs=num_classes)\n",
        "    cnn2.cuda()\n",
        "    print(cnn2.parameters)\n",
        "    optimizer2 = torch.optim.Adam(cnn2.parameters(), lr=learning_rate)\n",
        "\n",
        "    mean_pure_ratio1=0\n",
        "    mean_pure_ratio2=0\n",
        "\n",
        "    with open(txtfile, \"a\") as myfile:\n",
        "        myfile.write('epoch: train_acc1 train_acc2 test_acc1 test_acc2 pure_ratio1 pure_ratio2\\n')\n",
        "\n",
        "    epoch=0\n",
        "    train_acc1=0\n",
        "    train_acc2=0\n",
        "    # evaluate models with random weights\n",
        "    test_acc1, test_acc2=evaluate(test_loader, cnn1, cnn2)\n",
        "    print('Epoch [%d/%d] Test Accuracy on the %s test images: Model1 %.4f %% Model2 %.4f %% Pure Ratio1 %.4f %% Pure Ratio2 %.4f %%' % (epoch+1, args[\"n_epoch\"], len(test_dataset), test_acc1, test_acc2, mean_pure_ratio1, mean_pure_ratio2))\n",
        "    # save results\n",
        "    with open(txtfile, \"a\") as myfile:\n",
        "        myfile.write(str(int(epoch)) + ': '  + str(train_acc1) +' '  + str(train_acc2) +' '  + str(test_acc1) + \" \" + str(test_acc2) + ' '  + str(mean_pure_ratio1) + ' '  + str(mean_pure_ratio2) + \"\\n\")\n",
        "\n",
        "    # training\n",
        "    for epoch in range(1, args[\"n_epoch\"]):\n",
        "        # train models\n",
        "        cnn1.train()\n",
        "        adjust_learning_rate(optimizer1, epoch)\n",
        "        cnn2.train()\n",
        "        adjust_learning_rate(optimizer2, epoch)\n",
        "        train_acc1, train_acc2, pure_ratio_1_list, pure_ratio_2_list=train(train_loader, epoch, cnn1, optimizer1, cnn2, optimizer2)\n",
        "        # evaluate models\n",
        "        test_acc1, test_acc2=evaluate(test_loader, cnn1, cnn2)\n",
        "        # save results\n",
        "        mean_pure_ratio1 = sum(pure_ratio_1_list)/len(pure_ratio_1_list)\n",
        "        mean_pure_ratio2 = sum(pure_ratio_2_list)/len(pure_ratio_2_list)\n",
        "        print('Epoch [%d/%d] Test Accuracy on the %s test images: Model1 %.4f %% Model2 %.4f %%, Pure Ratio 1 %.4f %%, Pure Ratio 2 %.4f %%' % (epoch+1, args[\"n_epoch\"], len(test_dataset), test_acc1, test_acc2, mean_pure_ratio1, mean_pure_ratio2))\n",
        "        with open(txtfile, \"a\") as myfile:\n",
        "            myfile.write(str(int(epoch)) + ': '  + str(train_acc1) +' '  + str(train_acc2) +' '  + str(test_acc1) + \" \" + str(test_acc2) + ' ' + str(mean_pure_ratio1) + ' ' + str(mean_pure_ratio2) + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IggWWeie05KS",
        "outputId": "fc4903d2-e2f8-491d-c97b-e29a0c6f252f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n",
            "9 10\n",
            "50000\n",
            "Actual noise 0.80\n",
            "[[0.2        0.08888889 0.08888889 0.08888889 0.08888889 0.08888889\n",
            "  0.08888889 0.08888889 0.08888889 0.08888889]\n",
            " [0.08888889 0.2        0.08888889 0.08888889 0.08888889 0.08888889\n",
            "  0.08888889 0.08888889 0.08888889 0.08888889]\n",
            " [0.08888889 0.08888889 0.2        0.08888889 0.08888889 0.08888889\n",
            "  0.08888889 0.08888889 0.08888889 0.08888889]\n",
            " [0.08888889 0.08888889 0.08888889 0.2        0.08888889 0.08888889\n",
            "  0.08888889 0.08888889 0.08888889 0.08888889]\n",
            " [0.08888889 0.08888889 0.08888889 0.08888889 0.2        0.08888889\n",
            "  0.08888889 0.08888889 0.08888889 0.08888889]\n",
            " [0.08888889 0.08888889 0.08888889 0.08888889 0.08888889 0.2\n",
            "  0.08888889 0.08888889 0.08888889 0.08888889]\n",
            " [0.08888889 0.08888889 0.08888889 0.08888889 0.08888889 0.08888889\n",
            "  0.2        0.08888889 0.08888889 0.08888889]\n",
            " [0.08888889 0.08888889 0.08888889 0.08888889 0.08888889 0.08888889\n",
            "  0.08888889 0.2        0.08888889 0.08888889]\n",
            " [0.08888889 0.08888889 0.08888889 0.08888889 0.08888889 0.08888889\n",
            "  0.08888889 0.08888889 0.2        0.08888889]\n",
            " [0.08888889 0.08888889 0.08888889 0.08888889 0.08888889 0.08888889\n",
            "  0.08888889 0.08888889 0.08888889 0.2       ]]\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "main()"
      ],
      "metadata": {
        "id": "l2_KtTJa26qg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a705ff86-1458-4272-e8c4-a24167937327"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading dataset...\n",
            "building model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method Module.parameters of CNN(\n",
            "  (c1): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (c2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (c3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (c4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (c5): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (c6): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (l_c1): Linear(in_features=128, out_features=10, bias=True)\n",
            "  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")>\n",
            "<bound method Module.parameters of CNN(\n",
            "  (c1): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (c2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (c3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (c4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (c5): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (c6): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (l_c1): Linear(in_features=128, out_features=10, bias=True)\n",
            "  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")>\n",
            "Evaluating cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [1/25] Test Accuracy on the 10000 test images: Model1 10.1262 % Model2 10.0861 % Pure Ratio1 0.0000 % Pure Ratio2 0.0000 %\n",
            "Training cifar10_coteaching_symmetric_0.8...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/25], Iter [50/390] Training Accuracy1: 8.5938, Training Accuracy2: 7.8125, Loss1: 0.0199, Loss2: 0.0199, Pure Ratio1: 21.2414, Pure Ratio2 21.1034\n",
            "Epoch [2/25], Iter [100/390] Training Accuracy1: 12.5000, Training Accuracy2: 12.5000, Loss1: 0.0196, Loss2: 0.0197, Pure Ratio1: 20.8966, Pure Ratio2 20.8448\n",
            "Epoch [2/25], Iter [150/390] Training Accuracy1: 9.3750, Training Accuracy2: 10.1562, Loss1: 0.0194, Loss2: 0.0192, Pure Ratio1: 21.2069, Pure Ratio2 21.1034\n",
            "Epoch [2/25], Iter [200/390] Training Accuracy1: 19.5312, Training Accuracy2: 14.0625, Loss1: 0.0190, Loss2: 0.0191, Pure Ratio1: 21.3793, Pure Ratio2 21.3405\n",
            "Epoch [2/25], Iter [250/390] Training Accuracy1: 10.9375, Training Accuracy2: 13.2812, Loss1: 0.0198, Loss2: 0.0196, Pure Ratio1: 21.2655, Pure Ratio2 21.2207\n",
            "Epoch [2/25], Iter [300/390] Training Accuracy1: 10.9375, Training Accuracy2: 10.9375, Loss1: 0.0194, Loss2: 0.0192, Pure Ratio1: 21.1264, Pure Ratio2 21.0776\n",
            "Epoch [2/25], Iter [350/390] Training Accuracy1: 11.7188, Training Accuracy2: 13.2812, Loss1: 0.0192, Loss2: 0.0193, Pure Ratio1: 21.1946, Pure Ratio2 21.1502\n",
            "Evaluating cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [2/25] Test Accuracy on the 10000 test images: Model1 17.5180 % Model2 17.2376 %, Pure Ratio 1 21.1141 %, Pure Ratio 2 21.0787 %\n",
            "Training cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [3/25], Iter [50/390] Training Accuracy1: 10.9375, Training Accuracy2: 14.8438, Loss1: 0.0204, Loss2: 0.0204, Pure Ratio1: 23.6000, Pure Ratio2 23.1429\n",
            "Epoch [3/25], Iter [100/390] Training Accuracy1: 6.2500, Training Accuracy2: 9.3750, Loss1: 0.0206, Loss2: 0.0207, Pure Ratio1: 23.1143, Pure Ratio2 22.9619\n",
            "Epoch [3/25], Iter [150/390] Training Accuracy1: 9.3750, Training Accuracy2: 10.9375, Loss1: 0.0201, Loss2: 0.0201, Pure Ratio1: 22.5206, Pure Ratio2 22.4127\n",
            "Epoch [3/25], Iter [200/390] Training Accuracy1: 6.2500, Training Accuracy2: 11.7188, Loss1: 0.0205, Loss2: 0.0205, Pure Ratio1: 22.2048, Pure Ratio2 22.1095\n",
            "Epoch [3/25], Iter [250/390] Training Accuracy1: 10.1562, Training Accuracy2: 14.8438, Loss1: 0.0205, Loss2: 0.0204, Pure Ratio1: 22.1333, Pure Ratio2 22.0648\n",
            "Epoch [3/25], Iter [300/390] Training Accuracy1: 8.5938, Training Accuracy2: 14.8438, Loss1: 0.0206, Loss2: 0.0202, Pure Ratio1: 21.8889, Pure Ratio2 21.8286\n",
            "Epoch [3/25], Iter [350/390] Training Accuracy1: 14.8438, Training Accuracy2: 14.0625, Loss1: 0.0200, Loss2: 0.0202, Pure Ratio1: 21.6245, Pure Ratio2 21.6000\n",
            "Evaluating cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [3/25] Test Accuracy on the 10000 test images: Model1 14.9940 % Model2 15.9255 %, Pure Ratio 1 21.5897 %, Pure Ratio 2 21.5531 %\n",
            "Training cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [4/25], Iter [50/390] Training Accuracy1: 9.3750, Training Accuracy2: 10.1562, Loss1: 0.0230, Loss2: 0.0230, Pure Ratio1: 21.6774, Pure Ratio2 21.8710\n",
            "Epoch [4/25], Iter [100/390] Training Accuracy1: 10.9375, Training Accuracy2: 10.1562, Loss1: 0.0218, Loss2: 0.0219, Pure Ratio1: 21.8387, Pure Ratio2 21.9140\n",
            "Epoch [4/25], Iter [150/390] Training Accuracy1: 12.5000, Training Accuracy2: 8.5938, Loss1: 0.0216, Loss2: 0.0217, Pure Ratio1: 22.0215, Pure Ratio2 22.0645\n",
            "Epoch [4/25], Iter [200/390] Training Accuracy1: 10.9375, Training Accuracy2: 11.7188, Loss1: 0.0215, Loss2: 0.0216, Pure Ratio1: 22.0484, Pure Ratio2 22.0645\n",
            "Epoch [4/25], Iter [250/390] Training Accuracy1: 13.2812, Training Accuracy2: 11.7188, Loss1: 0.0215, Loss2: 0.0218, Pure Ratio1: 22.0860, Pure Ratio2 22.0989\n",
            "Epoch [4/25], Iter [300/390] Training Accuracy1: 7.0312, Training Accuracy2: 8.5938, Loss1: 0.0219, Loss2: 0.0219, Pure Ratio1: 21.9749, Pure Ratio2 22.0000\n",
            "Epoch [4/25], Iter [350/390] Training Accuracy1: 10.9375, Training Accuracy2: 5.4688, Loss1: 0.0211, Loss2: 0.0211, Pure Ratio1: 22.1598, Pure Ratio2 22.1905\n",
            "Evaluating cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [4/25] Test Accuracy on the 10000 test images: Model1 19.3209 % Model2 18.3293 %, Pure Ratio 1 22.2498 %, Pure Ratio 2 22.2581 %\n",
            "Training cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [5/25], Iter [50/390] Training Accuracy1: 9.3750, Training Accuracy2: 11.7188, Loss1: 0.0241, Loss2: 0.0239, Pure Ratio1: 25.0732, Pure Ratio2 24.8780\n",
            "Epoch [5/25], Iter [100/390] Training Accuracy1: 10.9375, Training Accuracy2: 14.0625, Loss1: 0.0234, Loss2: 0.0233, Pure Ratio1: 24.8902, Pure Ratio2 24.7195\n",
            "Epoch [5/25], Iter [150/390] Training Accuracy1: 8.5938, Training Accuracy2: 9.3750, Loss1: 0.0236, Loss2: 0.0234, Pure Ratio1: 24.3171, Pure Ratio2 24.2114\n",
            "Epoch [5/25], Iter [200/390] Training Accuracy1: 9.3750, Training Accuracy2: 10.1562, Loss1: 0.0225, Loss2: 0.0224, Pure Ratio1: 24.2073, Pure Ratio2 24.0915\n",
            "Epoch [5/25], Iter [250/390] Training Accuracy1: 10.1562, Training Accuracy2: 7.8125, Loss1: 0.0230, Loss2: 0.0230, Pure Ratio1: 24.3268, Pure Ratio2 24.1902\n",
            "Epoch [5/25], Iter [300/390] Training Accuracy1: 13.2812, Training Accuracy2: 14.0625, Loss1: 0.0236, Loss2: 0.0239, Pure Ratio1: 24.2033, Pure Ratio2 24.1301\n",
            "Epoch [5/25], Iter [350/390] Training Accuracy1: 11.7188, Training Accuracy2: 12.5000, Loss1: 0.0233, Loss2: 0.0233, Pure Ratio1: 24.3449, Pure Ratio2 24.3066\n",
            "Evaluating cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [5/25] Test Accuracy on the 10000 test images: Model1 17.9087 % Model2 17.1274 %, Pure Ratio 1 24.2214 %, Pure Ratio 2 24.1995 %\n",
            "Training cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [6/25], Iter [50/390] Training Accuracy1: 10.1562, Training Accuracy2: 9.3750, Loss1: 0.0249, Loss2: 0.0250, Pure Ratio1: 26.7042, Pure Ratio2 26.4789\n",
            "Epoch [6/25], Iter [100/390] Training Accuracy1: 14.0625, Training Accuracy2: 12.5000, Loss1: 0.0258, Loss2: 0.0258, Pure Ratio1: 24.9718, Pure Ratio2 24.8732\n",
            "Epoch [6/25], Iter [150/390] Training Accuracy1: 22.6562, Training Accuracy2: 15.6250, Loss1: 0.0238, Loss2: 0.0242, Pure Ratio1: 24.9296, Pure Ratio2 24.8920\n",
            "Epoch [6/25], Iter [200/390] Training Accuracy1: 8.5938, Training Accuracy2: 12.5000, Loss1: 0.0260, Loss2: 0.0259, Pure Ratio1: 24.4859, Pure Ratio2 24.5070\n",
            "Epoch [6/25], Iter [250/390] Training Accuracy1: 9.3750, Training Accuracy2: 10.9375, Loss1: 0.0241, Loss2: 0.0239, Pure Ratio1: 24.7718, Pure Ratio2 24.7887\n",
            "Epoch [6/25], Iter [300/390] Training Accuracy1: 8.5938, Training Accuracy2: 6.2500, Loss1: 0.0243, Loss2: 0.0246, Pure Ratio1: 24.8028, Pure Ratio2 24.8122\n",
            "Epoch [6/25], Iter [350/390] Training Accuracy1: 8.5938, Training Accuracy2: 10.1562, Loss1: 0.0245, Loss2: 0.0244, Pure Ratio1: 24.7243, Pure Ratio2 24.7082\n",
            "Evaluating cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [6/25] Test Accuracy on the 10000 test images: Model1 21.2340 % Model2 21.2139 %, Pure Ratio 1 24.6046 %, Pure Ratio 2 24.5901 %\n",
            "Training cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [7/25], Iter [50/390] Training Accuracy1: 17.9688, Training Accuracy2: 12.5000, Loss1: 0.0261, Loss2: 0.0263, Pure Ratio1: 25.9661, Pure Ratio2 26.0339\n",
            "Epoch [7/25], Iter [100/390] Training Accuracy1: 17.1875, Training Accuracy2: 13.2812, Loss1: 0.0246, Loss2: 0.0245, Pure Ratio1: 26.0339, Pure Ratio2 26.1695\n",
            "Epoch [7/25], Iter [150/390] Training Accuracy1: 12.5000, Training Accuracy2: 16.4062, Loss1: 0.0255, Loss2: 0.0254, Pure Ratio1: 26.3503, Pure Ratio2 26.4181\n",
            "Epoch [7/25], Iter [200/390] Training Accuracy1: 6.2500, Training Accuracy2: 10.1562, Loss1: 0.0267, Loss2: 0.0266, Pure Ratio1: 26.4407, Pure Ratio2 26.4322\n",
            "Epoch [7/25], Iter [250/390] Training Accuracy1: 14.8438, Training Accuracy2: 10.9375, Loss1: 0.0262, Loss2: 0.0269, Pure Ratio1: 26.3797, Pure Ratio2 26.4542\n",
            "Epoch [7/25], Iter [300/390] Training Accuracy1: 13.2812, Training Accuracy2: 10.1562, Loss1: 0.0260, Loss2: 0.0260, Pure Ratio1: 26.5254, Pure Ratio2 26.6102\n",
            "Epoch [7/25], Iter [350/390] Training Accuracy1: 7.0312, Training Accuracy2: 7.8125, Loss1: 0.0269, Loss2: 0.0269, Pure Ratio1: 26.2857, Pure Ratio2 26.3874\n",
            "Evaluating cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [7/25] Test Accuracy on the 10000 test images: Model1 15.6951 % Model2 14.9439 %, Pure Ratio 1 26.2364 %, Pure Ratio 2 26.3538 %\n",
            "Training cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [8/25], Iter [50/390] Training Accuracy1: 7.8125, Training Accuracy2: 8.5938, Loss1: 0.0300, Loss2: 0.0304, Pure Ratio1: 28.6667, Pure Ratio2 28.7917\n",
            "Epoch [8/25], Iter [100/390] Training Accuracy1: 8.5938, Training Accuracy2: 7.0312, Loss1: 0.0307, Loss2: 0.0312, Pure Ratio1: 28.7292, Pure Ratio2 28.9167\n",
            "Epoch [8/25], Iter [150/390] Training Accuracy1: 6.2500, Training Accuracy2: 8.5938, Loss1: 0.0303, Loss2: 0.0303, Pure Ratio1: 27.9583, Pure Ratio2 28.0139\n",
            "Epoch [8/25], Iter [200/390] Training Accuracy1: 11.7188, Training Accuracy2: 12.5000, Loss1: 0.0282, Loss2: 0.0281, Pure Ratio1: 27.9583, Pure Ratio2 27.9271\n",
            "Epoch [8/25], Iter [250/390] Training Accuracy1: 12.5000, Training Accuracy2: 12.5000, Loss1: 0.0280, Loss2: 0.0285, Pure Ratio1: 28.6000, Pure Ratio2 28.4917\n",
            "Epoch [8/25], Iter [300/390] Training Accuracy1: 16.4062, Training Accuracy2: 14.0625, Loss1: 0.0262, Loss2: 0.0266, Pure Ratio1: 28.7569, Pure Ratio2 28.5903\n",
            "Epoch [8/25], Iter [350/390] Training Accuracy1: 11.7188, Training Accuracy2: 10.1562, Loss1: 0.0261, Loss2: 0.0265, Pure Ratio1: 28.8036, Pure Ratio2 28.6310\n",
            "Evaluating cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [8/25] Test Accuracy on the 10000 test images: Model1 19.7015 % Model2 20.7031 %, Pure Ratio 1 28.7447 %, Pure Ratio 2 28.5791 %\n",
            "Training cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [9/25], Iter [50/390] Training Accuracy1: 8.5938, Training Accuracy2: 10.1562, Loss1: 0.0350, Loss2: 0.0350, Pure Ratio1: 31.5000, Pure Ratio2 31.1111\n",
            "Epoch [9/25], Iter [100/390] Training Accuracy1: 16.4062, Training Accuracy2: 13.2812, Loss1: 0.0295, Loss2: 0.0294, Pure Ratio1: 30.5833, Pure Ratio2 30.2778\n",
            "Epoch [9/25], Iter [150/390] Training Accuracy1: 9.3750, Training Accuracy2: 9.3750, Loss1: 0.0305, Loss2: 0.0309, Pure Ratio1: 30.5926, Pure Ratio2 30.3148\n",
            "Epoch [9/25], Iter [200/390] Training Accuracy1: 11.7188, Training Accuracy2: 14.0625, Loss1: 0.0317, Loss2: 0.0319, Pure Ratio1: 30.3194, Pure Ratio2 30.2361\n",
            "Epoch [9/25], Iter [250/390] Training Accuracy1: 11.7188, Training Accuracy2: 10.1562, Loss1: 0.0287, Loss2: 0.0290, Pure Ratio1: 29.9444, Pure Ratio2 29.9222\n",
            "Epoch [9/25], Iter [300/390] Training Accuracy1: 14.8438, Training Accuracy2: 13.2812, Loss1: 0.0290, Loss2: 0.0288, Pure Ratio1: 29.7130, Pure Ratio2 29.6852\n",
            "Epoch [9/25], Iter [350/390] Training Accuracy1: 10.9375, Training Accuracy2: 7.8125, Loss1: 0.0275, Loss2: 0.0280, Pure Ratio1: 29.7460, Pure Ratio2 29.7222\n",
            "Evaluating cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [9/25] Test Accuracy on the 10000 test images: Model1 15.8654 % Model2 15.2544 %, Pure Ratio 1 29.8362 %, Pure Ratio 2 29.8433 %\n",
            "Training cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [10/25], Iter [50/390] Training Accuracy1: 13.2812, Training Accuracy2: 14.8438, Loss1: 0.0280, Loss2: 0.0287, Pure Ratio1: 28.8000, Pure Ratio2 28.3200\n",
            "Epoch [10/25], Iter [100/390] Training Accuracy1: 15.6250, Training Accuracy2: 17.9688, Loss1: 0.0219, Loss2: 0.0230, Pure Ratio1: 30.2400, Pure Ratio2 29.7600\n",
            "Epoch [10/25], Iter [150/390] Training Accuracy1: 14.8438, Training Accuracy2: 10.9375, Loss1: 0.0289, Loss2: 0.0276, Pure Ratio1: 30.3467, Pure Ratio2 29.8400\n",
            "Epoch [10/25], Iter [200/390] Training Accuracy1: 10.1562, Training Accuracy2: 8.5938, Loss1: 0.0456, Loss2: 0.0430, Pure Ratio1: 30.9000, Pure Ratio2 30.5000\n",
            "Epoch [10/25], Iter [250/390] Training Accuracy1: 10.1562, Training Accuracy2: 9.3750, Loss1: 0.0295, Loss2: 0.0309, Pure Ratio1: 30.8960, Pure Ratio2 30.5280\n",
            "Epoch [10/25], Iter [300/390] Training Accuracy1: 12.5000, Training Accuracy2: 13.2812, Loss1: 0.0278, Loss2: 0.0282, Pure Ratio1: 31.0400, Pure Ratio2 30.7067\n",
            "Epoch [10/25], Iter [350/390] Training Accuracy1: 14.8438, Training Accuracy2: 16.4062, Loss1: 0.0211, Loss2: 0.0208, Pure Ratio1: 30.7314, Pure Ratio2 30.4800\n",
            "Evaluating cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [10/25] Test Accuracy on the 10000 test images: Model1 20.3826 % Model2 20.0220 %, Pure Ratio 1 30.7487 %, Pure Ratio 2 30.5641 %\n",
            "Training cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [11/25], Iter [50/390] Training Accuracy1: 8.5938, Training Accuracy2: 10.9375, Loss1: 0.0293, Loss2: 0.0285, Pure Ratio1: 31.5200, Pure Ratio2 31.7600\n",
            "Epoch [11/25], Iter [100/390] Training Accuracy1: 6.2500, Training Accuracy2: 6.2500, Loss1: 0.0391, Loss2: 0.0402, Pure Ratio1: 30.2400, Pure Ratio2 30.1600\n",
            "Epoch [11/25], Iter [150/390] Training Accuracy1: 11.7188, Training Accuracy2: 13.2812, Loss1: 0.0274, Loss2: 0.0255, Pure Ratio1: 30.1333, Pure Ratio2 30.0533\n",
            "Epoch [11/25], Iter [200/390] Training Accuracy1: 14.0625, Training Accuracy2: 13.2812, Loss1: 0.0285, Loss2: 0.0273, Pure Ratio1: 30.6600, Pure Ratio2 30.7200\n",
            "Epoch [11/25], Iter [250/390] Training Accuracy1: 14.8438, Training Accuracy2: 14.8438, Loss1: 0.0241, Loss2: 0.0241, Pure Ratio1: 31.1680, Pure Ratio2 31.2480\n",
            "Epoch [11/25], Iter [300/390] Training Accuracy1: 10.1562, Training Accuracy2: 10.1562, Loss1: 0.0311, Loss2: 0.0318, Pure Ratio1: 31.0933, Pure Ratio2 31.0933\n",
            "Epoch [11/25], Iter [350/390] Training Accuracy1: 12.5000, Training Accuracy2: 13.2812, Loss1: 0.0268, Loss2: 0.0259, Pure Ratio1: 30.8571, Pure Ratio2 30.8571\n",
            "Evaluating cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [11/25] Test Accuracy on the 10000 test images: Model1 19.2909 % Model2 19.3009 %, Pure Ratio 1 31.0154 %, Pure Ratio 2 31.0359 %\n",
            "Training cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [12/25], Iter [50/390] Training Accuracy1: 15.6250, Training Accuracy2: 14.8438, Loss1: 0.0225, Loss2: 0.0219, Pure Ratio1: 30.9600, Pure Ratio2 30.8800\n",
            "Epoch [12/25], Iter [100/390] Training Accuracy1: 11.7188, Training Accuracy2: 13.2812, Loss1: 0.0259, Loss2: 0.0248, Pure Ratio1: 31.6400, Pure Ratio2 31.8400\n",
            "Epoch [12/25], Iter [150/390] Training Accuracy1: 14.0625, Training Accuracy2: 10.9375, Loss1: 0.0275, Loss2: 0.0274, Pure Ratio1: 31.4933, Pure Ratio2 31.5200\n",
            "Epoch [12/25], Iter [200/390] Training Accuracy1: 13.2812, Training Accuracy2: 11.7188, Loss1: 0.0273, Loss2: 0.0270, Pure Ratio1: 31.6600, Pure Ratio2 31.6400\n",
            "Epoch [12/25], Iter [250/390] Training Accuracy1: 9.3750, Training Accuracy2: 10.1562, Loss1: 0.0346, Loss2: 0.0333, Pure Ratio1: 31.5840, Pure Ratio2 31.4880\n",
            "Epoch [12/25], Iter [300/390] Training Accuracy1: 9.3750, Training Accuracy2: 8.5938, Loss1: 0.0314, Loss2: 0.0327, Pure Ratio1: 31.6267, Pure Ratio2 31.6400\n",
            "Epoch [12/25], Iter [350/390] Training Accuracy1: 10.1562, Training Accuracy2: 11.7188, Loss1: 0.0303, Loss2: 0.0299, Pure Ratio1: 31.3600, Pure Ratio2 31.3714\n",
            "Evaluating cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [12/25] Test Accuracy on the 10000 test images: Model1 18.5597 % Model2 18.2392 %, Pure Ratio 1 31.4256 %, Pure Ratio 2 31.4256 %\n",
            "Training cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [13/25], Iter [50/390] Training Accuracy1: 9.3750, Training Accuracy2: 9.3750, Loss1: 0.0321, Loss2: 0.0325, Pure Ratio1: 31.1200, Pure Ratio2 30.8800\n",
            "Epoch [13/25], Iter [100/390] Training Accuracy1: 10.1562, Training Accuracy2: 10.9375, Loss1: 0.0304, Loss2: 0.0292, Pure Ratio1: 30.6000, Pure Ratio2 30.6000\n",
            "Epoch [13/25], Iter [150/390] Training Accuracy1: 10.9375, Training Accuracy2: 10.9375, Loss1: 0.0335, Loss2: 0.0349, Pure Ratio1: 32.1333, Pure Ratio2 32.0533\n",
            "Epoch [13/25], Iter [200/390] Training Accuracy1: 7.8125, Training Accuracy2: 8.5938, Loss1: 0.0810, Loss2: 0.0863, Pure Ratio1: 32.3600, Pure Ratio2 32.2600\n",
            "Epoch [13/25], Iter [250/390] Training Accuracy1: 13.2812, Training Accuracy2: 13.2812, Loss1: 0.0234, Loss2: 0.0239, Pure Ratio1: 32.4160, Pure Ratio2 32.3840\n",
            "Epoch [13/25], Iter [300/390] Training Accuracy1: 10.9375, Training Accuracy2: 13.2812, Loss1: 0.0283, Loss2: 0.0272, Pure Ratio1: 32.1467, Pure Ratio2 32.0800\n",
            "Epoch [13/25], Iter [350/390] Training Accuracy1: 9.3750, Training Accuracy2: 8.5938, Loss1: 0.0331, Loss2: 0.0323, Pure Ratio1: 32.0686, Pure Ratio2 32.0114\n",
            "Evaluating cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [13/25] Test Accuracy on the 10000 test images: Model1 18.8101 % Model2 19.2408 %, Pure Ratio 1 31.8462 %, Pure Ratio 2 31.7846 %\n",
            "Training cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [14/25], Iter [50/390] Training Accuracy1: 14.0625, Training Accuracy2: 13.2812, Loss1: 0.0262, Loss2: 0.0259, Pure Ratio1: 31.7600, Pure Ratio2 31.8400\n",
            "Epoch [14/25], Iter [100/390] Training Accuracy1: 8.5938, Training Accuracy2: 8.5938, Loss1: 0.0471, Loss2: 0.0464, Pure Ratio1: 32.2800, Pure Ratio2 32.0000\n",
            "Epoch [14/25], Iter [150/390] Training Accuracy1: 16.4062, Training Accuracy2: 15.6250, Loss1: 0.0212, Loss2: 0.0210, Pure Ratio1: 31.9200, Pure Ratio2 31.7067\n",
            "Epoch [14/25], Iter [200/390] Training Accuracy1: 10.9375, Training Accuracy2: 10.9375, Loss1: 0.0301, Loss2: 0.0320, Pure Ratio1: 31.6000, Pure Ratio2 31.5400\n",
            "Epoch [14/25], Iter [250/390] Training Accuracy1: 7.0312, Training Accuracy2: 8.5938, Loss1: 0.0381, Loss2: 0.0336, Pure Ratio1: 31.4880, Pure Ratio2 31.4240\n",
            "Epoch [14/25], Iter [300/390] Training Accuracy1: 12.5000, Training Accuracy2: 11.7188, Loss1: 0.0251, Loss2: 0.0257, Pure Ratio1: 31.7067, Pure Ratio2 31.6800\n",
            "Epoch [14/25], Iter [350/390] Training Accuracy1: 9.3750, Training Accuracy2: 7.8125, Loss1: 0.0432, Loss2: 0.0401, Pure Ratio1: 31.5429, Pure Ratio2 31.5886\n",
            "Evaluating cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [14/25] Test Accuracy on the 10000 test images: Model1 20.9435 % Model2 20.4327 %, Pure Ratio 1 31.5795 %, Pure Ratio 2 31.6205 %\n",
            "Training cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [15/25], Iter [50/390] Training Accuracy1: 13.2812, Training Accuracy2: 13.2812, Loss1: 0.0240, Loss2: 0.0231, Pure Ratio1: 32.5600, Pure Ratio2 32.4000\n",
            "Epoch [15/25], Iter [100/390] Training Accuracy1: 10.1562, Training Accuracy2: 10.9375, Loss1: 0.0292, Loss2: 0.0286, Pure Ratio1: 31.8000, Pure Ratio2 31.8400\n",
            "Epoch [15/25], Iter [150/390] Training Accuracy1: 12.5000, Training Accuracy2: 13.2812, Loss1: 0.0263, Loss2: 0.0251, Pure Ratio1: 31.2800, Pure Ratio2 31.4400\n",
            "Epoch [15/25], Iter [200/390] Training Accuracy1: 15.6250, Training Accuracy2: 14.8438, Loss1: 0.0239, Loss2: 0.0243, Pure Ratio1: 31.5800, Pure Ratio2 31.6600\n",
            "Epoch [15/25], Iter [250/390] Training Accuracy1: 10.9375, Training Accuracy2: 11.7188, Loss1: 0.0272, Loss2: 0.0280, Pure Ratio1: 31.1680, Pure Ratio2 31.2800\n",
            "Epoch [15/25], Iter [300/390] Training Accuracy1: 6.2500, Training Accuracy2: 7.0312, Loss1: 0.0740, Loss2: 0.0738, Pure Ratio1: 31.1067, Pure Ratio2 31.1067\n",
            "Epoch [15/25], Iter [350/390] Training Accuracy1: 10.1562, Training Accuracy2: 10.9375, Loss1: 0.0304, Loss2: 0.0294, Pure Ratio1: 31.2686, Pure Ratio2 31.3143\n",
            "Evaluating cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [15/25] Test Accuracy on the 10000 test images: Model1 20.0721 % Model2 20.5028 %, Pure Ratio 1 31.3436 %, Pure Ratio 2 31.4051 %\n",
            "Training cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [16/25], Iter [50/390] Training Accuracy1: 8.5938, Training Accuracy2: 10.9375, Loss1: 0.0408, Loss2: 0.0379, Pure Ratio1: 33.2800, Pure Ratio2 33.8400\n",
            "Epoch [16/25], Iter [100/390] Training Accuracy1: 14.8438, Training Accuracy2: 13.2812, Loss1: 0.0223, Loss2: 0.0234, Pure Ratio1: 32.8000, Pure Ratio2 33.1600\n",
            "Epoch [16/25], Iter [150/390] Training Accuracy1: 12.5000, Training Accuracy2: 10.9375, Loss1: 0.0289, Loss2: 0.0290, Pure Ratio1: 32.0000, Pure Ratio2 32.2667\n",
            "Epoch [16/25], Iter [200/390] Training Accuracy1: 14.0625, Training Accuracy2: 14.0625, Loss1: 0.0257, Loss2: 0.0250, Pure Ratio1: 31.9600, Pure Ratio2 32.2000\n",
            "Epoch [16/25], Iter [250/390] Training Accuracy1: 14.8438, Training Accuracy2: 13.2812, Loss1: 0.0270, Loss2: 0.0261, Pure Ratio1: 32.0000, Pure Ratio2 32.1280\n",
            "Epoch [16/25], Iter [300/390] Training Accuracy1: 13.2812, Training Accuracy2: 12.5000, Loss1: 0.0248, Loss2: 0.0256, Pure Ratio1: 32.1200, Pure Ratio2 32.2000\n",
            "Epoch [16/25], Iter [350/390] Training Accuracy1: 10.1562, Training Accuracy2: 8.5938, Loss1: 0.0300, Loss2: 0.0300, Pure Ratio1: 32.2971, Pure Ratio2 32.3086\n",
            "Evaluating cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [16/25] Test Accuracy on the 10000 test images: Model1 19.3510 % Model2 19.7516 %, Pure Ratio 1 31.8256 %, Pure Ratio 2 31.7949 %\n",
            "Training cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [17/25], Iter [50/390] Training Accuracy1: 9.3750, Training Accuracy2: 9.3750, Loss1: 0.0321, Loss2: 0.0330, Pure Ratio1: 30.4000, Pure Ratio2 30.2400\n",
            "Epoch [17/25], Iter [100/390] Training Accuracy1: 11.7188, Training Accuracy2: 14.0625, Loss1: 0.0298, Loss2: 0.0270, Pure Ratio1: 31.2000, Pure Ratio2 30.9200\n",
            "Epoch [17/25], Iter [150/390] Training Accuracy1: 10.9375, Training Accuracy2: 10.9375, Loss1: 0.0279, Loss2: 0.0277, Pure Ratio1: 31.0400, Pure Ratio2 30.9600\n",
            "Epoch [17/25], Iter [200/390] Training Accuracy1: 10.9375, Training Accuracy2: 11.7188, Loss1: 0.0252, Loss2: 0.0255, Pure Ratio1: 31.1200, Pure Ratio2 31.0200\n",
            "Epoch [17/25], Iter [250/390] Training Accuracy1: 12.5000, Training Accuracy2: 12.5000, Loss1: 0.0286, Loss2: 0.0285, Pure Ratio1: 31.2960, Pure Ratio2 31.3280\n",
            "Epoch [17/25], Iter [300/390] Training Accuracy1: 9.3750, Training Accuracy2: 10.1562, Loss1: 0.0314, Loss2: 0.0330, Pure Ratio1: 31.0533, Pure Ratio2 31.1600\n",
            "Epoch [17/25], Iter [350/390] Training Accuracy1: 10.9375, Training Accuracy2: 12.5000, Loss1: 0.0308, Loss2: 0.0305, Pure Ratio1: 30.8914, Pure Ratio2 31.0400\n",
            "Evaluating cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [17/25] Test Accuracy on the 10000 test images: Model1 20.4227 % Model2 20.3425 %, Pure Ratio 1 31.1282 %, Pure Ratio 2 31.2615 %\n",
            "Training cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [18/25], Iter [50/390] Training Accuracy1: 9.3750, Training Accuracy2: 10.1562, Loss1: 0.0501, Loss2: 0.0496, Pure Ratio1: 30.2400, Pure Ratio2 29.8400\n",
            "Epoch [18/25], Iter [100/390] Training Accuracy1: 10.1562, Training Accuracy2: 8.5938, Loss1: 0.0311, Loss2: 0.0302, Pure Ratio1: 31.1600, Pure Ratio2 30.8400\n",
            "Epoch [18/25], Iter [150/390] Training Accuracy1: 14.0625, Training Accuracy2: 14.0625, Loss1: 0.0261, Loss2: 0.0259, Pure Ratio1: 31.6533, Pure Ratio2 31.3333\n",
            "Epoch [18/25], Iter [200/390] Training Accuracy1: 13.2812, Training Accuracy2: 13.2812, Loss1: 0.0380, Loss2: 0.0369, Pure Ratio1: 31.2200, Pure Ratio2 31.1000\n",
            "Epoch [18/25], Iter [250/390] Training Accuracy1: 9.3750, Training Accuracy2: 7.8125, Loss1: 0.0378, Loss2: 0.0401, Pure Ratio1: 31.1520, Pure Ratio2 31.0080\n",
            "Epoch [18/25], Iter [300/390] Training Accuracy1: 13.2812, Training Accuracy2: 12.5000, Loss1: 0.0254, Loss2: 0.0253, Pure Ratio1: 31.4533, Pure Ratio2 31.2133\n",
            "Epoch [18/25], Iter [350/390] Training Accuracy1: 14.8438, Training Accuracy2: 14.8438, Loss1: 0.0229, Loss2: 0.0227, Pure Ratio1: 31.3829, Pure Ratio2 31.1886\n",
            "Evaluating cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [18/25] Test Accuracy on the 10000 test images: Model1 19.9319 % Model2 20.4127 %, Pure Ratio 1 31.2410 %, Pure Ratio 2 31.1179 %\n",
            "Training cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [19/25], Iter [50/390] Training Accuracy1: 7.0312, Training Accuracy2: 8.5938, Loss1: 0.0467, Loss2: 0.0461, Pure Ratio1: 30.6400, Pure Ratio2 30.8000\n",
            "Epoch [19/25], Iter [100/390] Training Accuracy1: 10.1562, Training Accuracy2: 10.1562, Loss1: 0.0294, Loss2: 0.0315, Pure Ratio1: 30.8800, Pure Ratio2 30.9200\n",
            "Epoch [19/25], Iter [150/390] Training Accuracy1: 7.8125, Training Accuracy2: 9.3750, Loss1: 0.0493, Loss2: 0.0471, Pure Ratio1: 30.1600, Pure Ratio2 30.2133\n",
            "Epoch [19/25], Iter [200/390] Training Accuracy1: 4.6875, Training Accuracy2: 5.4688, Loss1: 0.0740, Loss2: 0.0737, Pure Ratio1: 30.2600, Pure Ratio2 30.2200\n",
            "Epoch [19/25], Iter [250/390] Training Accuracy1: 14.8438, Training Accuracy2: 15.6250, Loss1: 0.0238, Loss2: 0.0232, Pure Ratio1: 30.3360, Pure Ratio2 30.2880\n",
            "Epoch [19/25], Iter [300/390] Training Accuracy1: 8.5938, Training Accuracy2: 7.8125, Loss1: 0.0337, Loss2: 0.0339, Pure Ratio1: 30.8000, Pure Ratio2 30.8267\n",
            "Epoch [19/25], Iter [350/390] Training Accuracy1: 10.9375, Training Accuracy2: 11.7188, Loss1: 0.0278, Loss2: 0.0271, Pure Ratio1: 30.8686, Pure Ratio2 30.9143\n",
            "Evaluating cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [19/25] Test Accuracy on the 10000 test images: Model1 20.7131 % Model2 20.8834 %, Pure Ratio 1 30.7179 %, Pure Ratio 2 30.7487 %\n",
            "Training cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [20/25], Iter [50/390] Training Accuracy1: 8.5938, Training Accuracy2: 6.2500, Loss1: 0.0342, Loss2: 0.0343, Pure Ratio1: 32.1600, Pure Ratio2 32.7200\n",
            "Epoch [20/25], Iter [100/390] Training Accuracy1: 12.5000, Training Accuracy2: 12.5000, Loss1: 0.0259, Loss2: 0.0254, Pure Ratio1: 29.9200, Pure Ratio2 30.1200\n",
            "Epoch [20/25], Iter [150/390] Training Accuracy1: 10.9375, Training Accuracy2: 8.5938, Loss1: 0.0338, Loss2: 0.0343, Pure Ratio1: 31.1733, Pure Ratio2 31.4933\n",
            "Epoch [20/25], Iter [200/390] Training Accuracy1: 7.8125, Training Accuracy2: 8.5938, Loss1: 0.0322, Loss2: 0.0318, Pure Ratio1: 31.3600, Pure Ratio2 31.6600\n",
            "Epoch [20/25], Iter [250/390] Training Accuracy1: 6.2500, Training Accuracy2: 3.1250, Loss1: 0.0468, Loss2: 0.0439, Pure Ratio1: 30.8000, Pure Ratio2 31.1520\n",
            "Epoch [20/25], Iter [300/390] Training Accuracy1: 11.7188, Training Accuracy2: 10.9375, Loss1: 0.0304, Loss2: 0.0338, Pure Ratio1: 31.1200, Pure Ratio2 31.3200\n",
            "Epoch [20/25], Iter [350/390] Training Accuracy1: 8.5938, Training Accuracy2: 10.1562, Loss1: 0.0324, Loss2: 0.0330, Pure Ratio1: 31.1886, Pure Ratio2 31.3371\n",
            "Evaluating cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [20/25] Test Accuracy on the 10000 test images: Model1 21.1739 % Model2 21.1739 %, Pure Ratio 1 31.1897 %, Pure Ratio 2 31.2718 %\n",
            "Training cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [21/25], Iter [50/390] Training Accuracy1: 14.0625, Training Accuracy2: 14.0625, Loss1: 0.0224, Loss2: 0.0232, Pure Ratio1: 32.8000, Pure Ratio2 32.8000\n",
            "Epoch [21/25], Iter [100/390] Training Accuracy1: 13.2812, Training Accuracy2: 12.5000, Loss1: 0.0272, Loss2: 0.0262, Pure Ratio1: 33.1200, Pure Ratio2 33.0400\n",
            "Epoch [21/25], Iter [150/390] Training Accuracy1: 8.5938, Training Accuracy2: 10.1562, Loss1: 0.0440, Loss2: 0.0447, Pure Ratio1: 32.1600, Pure Ratio2 32.1333\n",
            "Epoch [21/25], Iter [200/390] Training Accuracy1: 6.2500, Training Accuracy2: 9.3750, Loss1: 0.0481, Loss2: 0.0453, Pure Ratio1: 32.2600, Pure Ratio2 32.2400\n",
            "Epoch [21/25], Iter [250/390] Training Accuracy1: 13.2812, Training Accuracy2: 13.2812, Loss1: 0.0253, Loss2: 0.0250, Pure Ratio1: 32.4480, Pure Ratio2 32.4320\n",
            "Epoch [21/25], Iter [300/390] Training Accuracy1: 9.3750, Training Accuracy2: 10.1562, Loss1: 0.0318, Loss2: 0.0319, Pure Ratio1: 32.1467, Pure Ratio2 32.1333\n",
            "Epoch [21/25], Iter [350/390] Training Accuracy1: 14.8438, Training Accuracy2: 14.0625, Loss1: 0.0243, Loss2: 0.0249, Pure Ratio1: 31.8171, Pure Ratio2 31.8400\n",
            "Evaluating cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [21/25] Test Accuracy on the 10000 test images: Model1 20.2624 % Model2 20.2825 %, Pure Ratio 1 31.7949 %, Pure Ratio 2 31.8256 %\n",
            "Training cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [22/25], Iter [50/390] Training Accuracy1: 14.8438, Training Accuracy2: 14.0625, Loss1: 0.0245, Loss2: 0.0244, Pure Ratio1: 32.1600, Pure Ratio2 32.2400\n",
            "Epoch [22/25], Iter [100/390] Training Accuracy1: 8.5938, Training Accuracy2: 9.3750, Loss1: 0.0363, Loss2: 0.0358, Pure Ratio1: 32.8400, Pure Ratio2 32.4400\n",
            "Epoch [22/25], Iter [150/390] Training Accuracy1: 8.5938, Training Accuracy2: 10.1562, Loss1: 0.0295, Loss2: 0.0287, Pure Ratio1: 32.1067, Pure Ratio2 31.7867\n",
            "Epoch [22/25], Iter [200/390] Training Accuracy1: 7.0312, Training Accuracy2: 6.2500, Loss1: 0.0683, Loss2: 0.0701, Pure Ratio1: 31.9400, Pure Ratio2 31.6600\n",
            "Epoch [22/25], Iter [250/390] Training Accuracy1: 11.7188, Training Accuracy2: 15.6250, Loss1: 0.0248, Loss2: 0.0240, Pure Ratio1: 32.1600, Pure Ratio2 31.9520\n",
            "Epoch [22/25], Iter [300/390] Training Accuracy1: 13.2812, Training Accuracy2: 12.5000, Loss1: 0.0303, Loss2: 0.0307, Pure Ratio1: 31.7333, Pure Ratio2 31.6000\n",
            "Epoch [22/25], Iter [350/390] Training Accuracy1: 11.7188, Training Accuracy2: 12.5000, Loss1: 0.0286, Loss2: 0.0284, Pure Ratio1: 31.6686, Pure Ratio2 31.6114\n",
            "Evaluating cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [22/25] Test Accuracy on the 10000 test images: Model1 20.6130 % Model2 20.9736 %, Pure Ratio 1 31.8564 %, Pure Ratio 2 31.8359 %\n",
            "Training cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [23/25], Iter [50/390] Training Accuracy1: 14.8438, Training Accuracy2: 13.2812, Loss1: 0.0243, Loss2: 0.0256, Pure Ratio1: 32.4800, Pure Ratio2 32.1600\n",
            "Epoch [23/25], Iter [100/390] Training Accuracy1: 12.5000, Training Accuracy2: 10.9375, Loss1: 0.0262, Loss2: 0.0282, Pure Ratio1: 31.4800, Pure Ratio2 31.0400\n",
            "Epoch [23/25], Iter [150/390] Training Accuracy1: 10.1562, Training Accuracy2: 10.1562, Loss1: 0.0330, Loss2: 0.0330, Pure Ratio1: 31.3867, Pure Ratio2 31.1467\n",
            "Epoch [23/25], Iter [200/390] Training Accuracy1: 6.2500, Training Accuracy2: 6.2500, Loss1: 0.0469, Loss2: 0.0425, Pure Ratio1: 32.1400, Pure Ratio2 31.8800\n",
            "Epoch [23/25], Iter [250/390] Training Accuracy1: 11.7188, Training Accuracy2: 10.1562, Loss1: 0.0328, Loss2: 0.0320, Pure Ratio1: 31.4720, Pure Ratio2 31.3440\n",
            "Epoch [23/25], Iter [300/390] Training Accuracy1: 12.5000, Training Accuracy2: 11.7188, Loss1: 0.0278, Loss2: 0.0283, Pure Ratio1: 31.5200, Pure Ratio2 31.3733\n",
            "Epoch [23/25], Iter [350/390] Training Accuracy1: 14.8438, Training Accuracy2: 17.1875, Loss1: 0.0242, Loss2: 0.0221, Pure Ratio1: 31.3829, Pure Ratio2 31.2000\n",
            "Evaluating cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [23/25] Test Accuracy on the 10000 test images: Model1 21.3542 % Model2 21.5144 %, Pure Ratio 1 31.5282 %, Pure Ratio 2 31.3846 %\n",
            "Training cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [24/25], Iter [50/390] Training Accuracy1: 11.7188, Training Accuracy2: 10.9375, Loss1: 0.0279, Loss2: 0.0264, Pure Ratio1: 32.0000, Pure Ratio2 32.0800\n",
            "Epoch [24/25], Iter [100/390] Training Accuracy1: 12.5000, Training Accuracy2: 13.2812, Loss1: 0.0265, Loss2: 0.0236, Pure Ratio1: 32.1200, Pure Ratio2 32.0800\n",
            "Epoch [24/25], Iter [150/390] Training Accuracy1: 9.3750, Training Accuracy2: 10.9375, Loss1: 0.0286, Loss2: 0.0277, Pure Ratio1: 32.2667, Pure Ratio2 32.1333\n",
            "Epoch [24/25], Iter [200/390] Training Accuracy1: 10.9375, Training Accuracy2: 10.9375, Loss1: 0.0313, Loss2: 0.0331, Pure Ratio1: 31.9000, Pure Ratio2 31.7800\n",
            "Epoch [24/25], Iter [250/390] Training Accuracy1: 14.0625, Training Accuracy2: 11.7188, Loss1: 0.0308, Loss2: 0.0304, Pure Ratio1: 32.2720, Pure Ratio2 32.1440\n",
            "Epoch [24/25], Iter [300/390] Training Accuracy1: 12.5000, Training Accuracy2: 13.2812, Loss1: 0.0285, Loss2: 0.0283, Pure Ratio1: 32.2800, Pure Ratio2 32.1733\n",
            "Epoch [24/25], Iter [350/390] Training Accuracy1: 10.9375, Training Accuracy2: 11.7188, Loss1: 0.0298, Loss2: 0.0284, Pure Ratio1: 32.2286, Pure Ratio2 32.2171\n",
            "Evaluating cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [24/25] Test Accuracy on the 10000 test images: Model1 20.4026 % Model2 20.6330 %, Pure Ratio 1 32.3590 %, Pure Ratio 2 32.2872 %\n",
            "Training cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [25/25], Iter [50/390] Training Accuracy1: 14.8438, Training Accuracy2: 14.8438, Loss1: 0.0225, Loss2: 0.0225, Pure Ratio1: 32.6400, Pure Ratio2 32.4000\n",
            "Epoch [25/25], Iter [100/390] Training Accuracy1: 14.8438, Training Accuracy2: 14.0625, Loss1: 0.0228, Loss2: 0.0226, Pure Ratio1: 31.8400, Pure Ratio2 32.0000\n",
            "Epoch [25/25], Iter [150/390] Training Accuracy1: 12.5000, Training Accuracy2: 13.2812, Loss1: 0.0269, Loss2: 0.0273, Pure Ratio1: 32.3467, Pure Ratio2 32.6400\n",
            "Epoch [25/25], Iter [200/390] Training Accuracy1: 12.5000, Training Accuracy2: 14.0625, Loss1: 0.0227, Loss2: 0.0232, Pure Ratio1: 32.8400, Pure Ratio2 33.0800\n",
            "Epoch [25/25], Iter [250/390] Training Accuracy1: 10.9375, Training Accuracy2: 11.7188, Loss1: 0.0324, Loss2: 0.0378, Pure Ratio1: 33.2800, Pure Ratio2 33.4560\n",
            "Epoch [25/25], Iter [300/390] Training Accuracy1: 12.5000, Training Accuracy2: 10.1562, Loss1: 0.0287, Loss2: 0.0293, Pure Ratio1: 32.9333, Pure Ratio2 33.0400\n",
            "Epoch [25/25], Iter [350/390] Training Accuracy1: 7.8125, Training Accuracy2: 10.1562, Loss1: 0.0437, Loss2: 0.0397, Pure Ratio1: 32.6743, Pure Ratio2 32.8114\n",
            "Evaluating cifar10_coteaching_symmetric_0.8...\n",
            "Epoch [25/25] Test Accuracy on the 10000 test images: Model1 20.9635 % Model2 21.0837 %, Pure Ratio 1 32.5641 %, Pure Ratio 2 32.7282 %\n",
            "CPU times: user 2h 13min 59s, sys: 41.2 s, total: 2h 14min 41s\n",
            "Wall time: 2h 15min 22s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "1. https://arxiv.org/abs/1804.06872\n",
        "2. https://github.com/bhanML/Co-teaching\n",
        "3. https://github.com/cleanlab/cleanlab\n",
        "4. https://github.com/yeachan-kr/pytorch-coteaching"
      ],
      "metadata": {
        "id": "bNAz4-TqwESA"
      }
    }
  ]
}